# -*- coding: utf-8 -*-
"""Transformer_Train.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1zrswcKNjSOJ6pK6cVl9qFJjdXa7N4Y59
"""

import numpy as np
import pandas as pd
import torch
import torch.nn as nn
from torch.utils.data import DataLoader, Dataset
from sklearn.preprocessing import MinMaxScaler

# Load and preprocess the dataset
file_path = '/content/drive/MyDrive/Sentiment Analysis/merged_data_for_transformer.csv'
data = pd.read_csv(file_path)

# Features and target
features = ['positive', 'neutral', 'negative', 'score', 'Open']
target = 'Close'

# Normalize the data
scaler = MinMaxScaler()
data[features + [target]] = scaler.fit_transform(data[features + [target]])

# Create a custom dataset class
class TimeSeriesDataset(Dataset):
    def __init__(self, data, features, target, sequence_length):
        self.data = data
        self.features = features
        self.target = target
        self.sequence_length = sequence_length

    def __len__(self):
        return len(self.data) - self.sequence_length

    def __getitem__(self, idx):
        x = self.data[self.features].iloc[idx:idx + self.sequence_length].values
        y = self.data[self.target].iloc[idx + self.sequence_length]
        return torch.tensor(x, dtype=torch.float32), torch.tensor(y, dtype=torch.float32)

# Hyperparameters
sequence_length = 3
batch_size = 16
learning_rate = 0.00001
epochs = 5

# Prepare the dataset
dataset = TimeSeriesDataset(data, features, target, sequence_length)
train_size = int(0.8 * len(dataset))
val_size = len(dataset) - train_size
train_dataset, val_dataset = torch.utils.data.random_split(dataset, [train_size, val_size])

train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)
val_loader = DataLoader(val_dataset, batch_size=batch_size, shuffle=False)

# Define the transformer model
class TransformerModel(nn.Module):
    def __init__(self, input_dim, embed_dim, num_heads, num_layers, dropout):
        super(TransformerModel, self).__init__()
        self.embedding = nn.Linear(input_dim, embed_dim)
        self.positional_encoding = nn.Parameter(torch.zeros(1, sequence_length, embed_dim))
        self.transformer = nn.TransformerEncoder(
            nn.TransformerEncoderLayer(embed_dim, num_heads, dropout=dropout),
            num_layers
        )
        self.fc = nn.Linear(embed_dim, 1)

    def forward(self, x):
        x = self.embedding(x) + self.positional_encoding
        x = self.transformer(x)
        x = x.mean(dim=1)
        return self.fc(x)

# Model instantiation
input_dim = len(features)
embed_dim = 64
num_heads = 4
num_layers = 2
dropout = 0.1

model = TransformerModel(input_dim, embed_dim, num_heads, num_layers, dropout)
criterion = nn.MSELoss()
optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate)

# Training loop
for epoch in range(epochs):
    model.train()
    train_loss = 0
    for x_batch, y_batch in train_loader:
        optimizer.zero_grad()
        output = model(x_batch)
        loss = criterion(output.squeeze(), y_batch)
        loss.backward()
        optimizer.step()
        train_loss += loss.item()

    model.eval()
    val_loss = 0
    with torch.no_grad():
        for x_batch, y_batch in val_loader:
            output = model(x_batch)
            loss = criterion(output.squeeze(), y_batch)
            val_loss += loss.item()

    print(f"Epoch {epoch + 1}/{epochs}, Train Loss: {train_loss / len(train_loader):.4f}, Val Loss: {val_loss / len(val_loader):.4f}")

# Save the model
torch.save(model.state_dict(), '/content/drive/MyDrive/NLP_Project/transformer_model.pth')

import matplotlib.pyplot as plt
from sklearn.metrics import mean_absolute_error, mean_squared_error, r2_score
import numpy as np

# Evaluate the model on the test/validation set
model.eval()
actuals = []
predictions = []

with torch.no_grad():
    for x_batch, y_batch in val_loader:
        preds = model(x_batch).squeeze()
        predictions.extend(preds.numpy())
        actuals.extend(y_batch.numpy())

# Calculate metrics
mae = mean_absolute_error(actuals, predictions)
mse = mean_squared_error(actuals, predictions)
rmse = np.sqrt(mse)
r2 = r2_score(actuals, predictions)

print(f"MAE: {mae:.4f}, MSE: {mse:.4f}, RMSE: {rmse:.4f}, RÂ²: {r2:.4f}")

# Plot Actual vs. Predicted
plt.figure(figsize=(20, 6))
plt.plot(actuals, label='Actual', alpha=0.7)
plt.plot(predictions, label='Predicted', alpha=0.7)
plt.legend()
plt.title('^DJI Actual vs. Predicted Closing Prices')
plt.xlabel('Time')
plt.ylabel('Normalized Price')
plt.show()

residuals = np.array(actuals) - np.array(predictions)
plt.figure(figsize=(8, 5))
plt.scatter(range(len(residuals)), residuals, alpha=0.5)
plt.axhline(0, color='red', linestyle='--')
plt.title('Residuals')
plt.xlabel('Index')
plt.ylabel('Residual')
plt.show()

plt.hist(residuals, bins=30, alpha=0.7)
plt.title('Distribution of Residuals')
plt.xlabel('Residual')
plt.ylabel('Frequency')
plt.show()

