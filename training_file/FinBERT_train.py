# -*- coding: utf-8 -*-
"""NLP_Project_Train.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1cFEFUUoh2XN94xDvpCdR2Q_I3N7WbfZa
"""

pip install transformers datasets torch

from transformers import AutoTokenizer, AutoModelForSequenceClassification

# Load tokenizer and model
model_name = "StephanAkkerman/FinTwitBERT-sentiment"
tokenizer = AutoTokenizer.from_pretrained(model_name)
model = AutoModelForSequenceClassification.from_pretrained(model_name, num_labels=3)

from google.colab import drive
drive.mount('/content/drive')

import pandas as pd
from datasets import Dataset

# Load dataset
data = pd.read_csv("/content/drive/MyDrive/NLP Project/data.csv")

# Map sentiment labels to numeric values
label_mapping = {"neutral": 0, "positive": 1, "negative": 2}
data["label"] = data["Sentiment"].map(label_mapping)

# Convert to Hugging Face dataset format
dataset = Dataset.from_pandas(data)

# Converts each sentence into token IDs using the pre-trained tokenizer.
def tokenize_function(example):
    return tokenizer(example["Sentence"], truncation=True, padding="max_length", max_length=256)

tokenized_dataset = dataset.map(tokenize_function, batched=True)

from sklearn.model_selection import train_test_split

# Split data
train_test_split = tokenized_dataset.train_test_split(test_size=0.2, seed=42)
train_dataset = train_test_split["train"]
val_dataset = train_test_split["test"]

from transformers import TrainingArguments, Trainer

# Define training arguments
training_args = TrainingArguments(
    output_dir="./results",          # Output directory
    eval_strategy="epoch",           # Evaluate every epoch
    save_strategy="epoch",           # Save checkpoint every epoch
    logging_dir="./logs",            # Log directory
    per_device_train_batch_size=8,   # Batch size for training
    per_device_eval_batch_size=8,   # Batch size for evaluation
    num_train_epochs=4,              # Number of training epochs
    learning_rate=1e-5,              # Learning rate
    weight_decay=0.1,               # Weight decay
    save_total_limit=1,              # Limit saved checkpoints
)

# Define the trainer
trainer = Trainer(
    model=model,
    args=training_args,
    train_dataset=train_dataset,
    eval_dataset=val_dataset,
    tokenizer=tokenizer,
)

trainer.train()

# Specify the directory to save the model and tokenizer
save_directory = "/content/drive/MyDrive/NLP_Project/FinTwitBERT_finetuned"

# Save the fine-tuned model
model.save_pretrained(save_directory)

# Save the tokenizer
tokenizer.save_pretrained(save_directory)

print(f"Model and tokenizer saved to {save_directory}")

from transformers import AutoTokenizer, AutoModelForSequenceClassification

# Load the fine-tuned model and tokenizer
saved_directory = "/content/drive/MyDrive/NLP_Project/FinTwitBERT_finetuned"
tokenizer = AutoTokenizer.from_pretrained(saved_directory)
model = AutoModelForSequenceClassification.from_pretrained(saved_directory)

print("Model and tokenizer successfully loaded from saved directory.")

pip install huggingface_hub

huggingface-cli login

from huggingface_hub import upload_folder

upload_folder(
    folder_path="/content/drive/MyDrive/NLP_Project/FinTwitBERT_finetuned",
    repo_id="your_username/finbert-finetuned",
    repo_type="model"
)

from transformers import AutoTokenizer, AutoModelForSequenceClassification

model_name = "your_username/finbert-finetuned"
tokenizer = AutoTokenizer.from_pretrained(model_name)
model = AutoModelForSequenceClassification.from_pretrained(model_name)
